{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: RAG Application Tutorial\n",
    "\n",
    "This notebook will guide you through building a Retrieval-Augmented Generation (RAG) application. The application will scrape information from a website, store it in a local vector database, and provide a Gradio interface for users to ask questions and receive relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "First, we need to install the necessary libraries for web scraping, vector database, and Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community langchain langchain_openai faiss-cpu gradio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define website you would like to scrape and enter your OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "website = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Web Scraping\n",
    "\n",
    "We will use Langchain Community Web Based Loader to load information from a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(website)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Text splitting\n",
    "\n",
    "We will use a text splitter to split the website in to chunks of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=100,\n",
    "    \n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sd.page_content for sd in split_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Store Information in a Local Vector Database\n",
    "\n",
    "We will use FAISS to create a local vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create function to retrieve information and to generate response from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "prompt = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
    "Use the context below to answer the question. If the answer is not in the context, say \"I don't know\".\"\"\"\n",
    "\n",
    "def answer_question(question, history):\n",
    "    relevant_documents = retriever.get_relevant_documents(question)\n",
    "\n",
    "    texts = [doc.page_content for doc in relevant_documents]\n",
    "    text_string = \"\\n\".join(texts)\n",
    "    question_prompt = f\"{prompt}\\n\\nContext:\\n{text_string}\\n\\nQuestion: {question}\"\n",
    "    llm_response = llm(question_prompt).content\n",
    "    return llm_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create a simple gradio UI to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "iface = gr.ChatInterface(\n",
    "    fn=answer_question,\n",
    "    title=\"RAG Application\",\n",
    "    description=\"Ask a question and get an answer based on the website.\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Turning our RAG system agentic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Routing\n",
    "Add an llm that decides which tool to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "# Your goal\n",
    "You are a assistant responsible for helping users create new material based on information from a website.\n",
    "\n",
    "# Instructions\n",
    "Depending on the user query you will determine whether they are asking a question that is related to finding information from the website or if they are asking for help creating new material. \n",
    "\n",
    "If they are asking a question, you will use the find_information tool to find the answer. \n",
    "\n",
    "If they are asking for help creating new material, you will use the write_content tool to help them create that material.\n",
    "\n",
    "# Your output\n",
    "You will respond with the output format below. You will only respond with the output format and nothing else.\n",
    "<tool>{enter chosen tool name here}</tool>\n",
    "\"\"\"\n",
    "\n",
    "def decide_tool(question):\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "    llm_response = llm(instructions + question).content\n",
    "    print(\"LLM Response:\", llm_response)\n",
    "    tool_name = llm_response.split(\"<tool>\")[1].split(\"</tool>\")[0]\n",
    "    return tool_name.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decide_tool(\"What is the capital of France?\")\n",
    "decide_tool(\"Can you help me write a poem about the ocean?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add new tool\n",
    "Add the tool for writing content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_content(write_content_instructions, history):\n",
    "    found_information = [h[1] for h in history]\n",
    "    llm_response = llm(write_content_instructions + \"\\n\".join(found_information)).content\n",
    "    return llm_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Combine\n",
    "Bring both tools in to one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_llm(question, history):\n",
    "    tool_name = decide_tool(question)\n",
    "    if tool_name == \"find_information\":\n",
    "        return answer_question(question, history)\n",
    "    elif tool_name == \"write_content\":\n",
    "        return write_content(question, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "iface = gr.ChatInterface(\n",
    "    fn=agentic_llm,\n",
    "    title=\"Agentic LLM Application\",\n",
    "    description=\"Retrieve information and when happy create new material based on it.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
