{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Application Tutorial\n",
    "\n",
    "This notebook will guide you through building a Retrieval-Augmented Generation (RAG) application. The application will scrape information from a website, store it in a local vector database, and provide a Gradio interface for users to ask questions and receive relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "First, we need to install the necessary libraries for web scraping, vector database, and Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community langchain langchain_openai faiss-cpu gradio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define website you would like to scrape and enter your OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "website = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Web Scraping\n",
    "\n",
    "We will use Langchain Community Web Based Loader to load information from a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(website)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Text splitting\n",
    "\n",
    "We will use a text splitter to split the website in to chunks of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=100,\n",
    "    \n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sd.page_content for sd in split_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Store Information in a Local Vector Database\n",
    "\n",
    "We will use FAISS to create a local vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create function to retrieve information and to generate response from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "prompt = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
    "Use the context below to answer the question. If the answer is not in the context, say \"I don't know\".\"\"\"\n",
    "\n",
    "def answer_question(question, history=[]):\n",
    "    relevant_documents = retriever.get_relevant_documents(question)\n",
    "\n",
    "    texts = [doc.page_content for doc in relevant_documents]\n",
    "    text_string = \"\\n\".join(texts)\n",
    "    question_prompt = f\"{prompt}\\n\\nContext:\\n{text_string}\\n\\nQuestion: {question}\"\n",
    "\n",
    "    llm_response = llm(question_prompt).content\n",
    "    return llm_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create a simple gradio UI to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "iface = gr.ChatInterface(\n",
    "    fn=answer_question,\n",
    "    title=\"RAG Application\",\n",
    "    description=\"Ask a question and get an answer based on the website.\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
